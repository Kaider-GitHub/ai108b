
    人工智慧是一個非常熱門的新興項目，未來應用的場景也非常廣泛，但人工智慧訓練完成的模型以及學習模式，往往可能突破人類現有的認知，
當然這種結果有利有弊，有利的地方在於可以協助人類進步現有的技術，啟發新點子，但弊端則在於如果模型的學習模式與認知與人類所認知的場景
不同，可能AI在某些狀況下，會無法發揮正常的運作能力。
    從網站中的資料中可以發現，雖然AI與對手在正常情況下互有勝負，但只要對手「出現異常狀況」例如不主動防守，目標AI也會變得失去目標，
甚至出現異常的情形，成功率就變低許多。我個人推測是因為它在訓練中的對手AI都是「正常AI」也就是會主動去防守，運作正常的AI，基於這個所
訓練出來的進攻AI，它所學會的並不是「主動贏得比賽」而是比較被動的「對抗對手的阻擋」來達到成功進攻的「結果」，所以在對手不正常時，這
個模型就「失能」了。如果這個問題應用到的是已經商用的AI，那麼在實際使用中如果因為有人惡意使用類似的對抗政策手段，可能就會讓這個AI失
效，因此我認為，對抗政策是一個AI應用中，需要維護的部分，也跟AI的資訊安全息息相關。
    人工智慧真的能達到人類甚至超越人類的水準，我想還要一些時間，因此我認為AI暫時應該被拿來只應用在輔助使用，而不是一味的追求全AI
化，如果AI完全獨立自主的運作，沒有監督的化，可能就會跟小孩子一樣，有可能被其他同儕(其他AI)影響，導致功能偏差，或學會偷吃步而不是完
整步驟等，有可能會導致輸出結果的不同。另外，透過這個實驗結果也告訴我們，訓練AI時，要設想的周詳，以這個實驗來舉例就是，在一開始就要
放幾個不同的模型，例如對抗政策的「對手失能」模型，或是有外來的球干擾啊，第三者入侵之類的狀況納入，這樣訓練出來的AI在功能上，就會比
較完善，也比較能抵抗對抗政策的攻擊。
    人工智慧的路還很漫長，我想未來人工智慧絕對是資訊安全領域的一大重要目標，不管是在供或防上，而且解決這類「資安問題」從根本上就是
在訓練時就調整好，以應付現實社會中各種複雜的狀況。現在的AI類比到現實社會上，就是小朋友，是還沒有出社會的初生之犢，因此如果沒有適當
環境引導它，我想他是無法勝任「代替」人類的這項預想的。
